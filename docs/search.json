[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nDiffEdit Paper Implementation\n\n\n\nStable Diffusion\n\nPaper Implementation\n\n\n\nThis was more mind-bending than I expected üòÖ. Watch me as I make it seem so simple in this blog üòé\n\n\n\n\n\nMay 4, 2024\n\n\nSuchit G\n\n\n\n\n\n\n\n\n\n\n\n\nTabularModel Deep Dive\n\n\n\nTabular\n\nNeural Networks\n\nfast.ai\n\n\n\nA detailed explanation of the fast.ai TabularModel class.\n\n\n\n\n\nDec 2, 2023\n\n\nSuchit G\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to NLP\n\n\n\nBeginner\n\nNLP\n\n\n\nSome fundamental terms and concepts related to NLP/LMs.\n\n\n\n\n\nOct 13, 2023\n\n\nSuchit G\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying handwritten digits (THE MNIST!)\n\n\n\nNeural Networks\n\nfast.ai\n\nComputerVision\n\n\n\nUnderstand the fundamental structure of a ML project by building a simple neural network on the MNIST dataset!\n\n\n\n\n\nSep 9, 2023\n\n\nSuchit G\n\n\n\n\n\n\n\n\n\n\n\n\nData Cleaning and Augmentation\n\n\n\nData Processing\n\nBeginner\n\n\n\nThe counter-intuitive idea of data cleaning after training a model and creating data by ourselves.\n\n\n\n\n\nJun 27, 2023\n\n\nSuchit G\n\n\n\n\n\n\n\n\n\n\n\n\nTrain your first image classifier (AI) model!\n\n\n\nBeginner\n\nComputerVision\n\nfast.ai\n\n\n\nGet a taste of AI/deep learning with this simple tutorial that trains an image classifier model!\n\n\n\n\n\nMay 29, 2023\n\n\nSuchit G\n\n\n\n\n\n\n\n\n\n\n\n\nNew to CS/tech? Here‚Äôs my journey so far\n\n\n\nExperience\n\nAdvice\n\nJourney\n\n\n\nIt‚Äôs been more than two years since the first time I got lustfully ü§§ attracted to CS‚Ä¶\n\n\n\n\n\nMay 18, 2023\n\n\nSuchit G\n\n\n\n\n\n\n\n\n\n\n\n\nHey yo! Welcome to my blog peeps!\n\n\n\nExperience\n\n\n\nHiya! Know more about my blog!\n\n\n\n\n\nMay 15, 2023\n\n\nSuchit G\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nHarlow Malloc\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "bits/testtimecompute/index.html",
    "href": "bits/testtimecompute/index.html",
    "title": "Test-time compute, but for humans.",
    "section": "",
    "text": "The way O1 works is cool.\nResearchers have been figuring out that you can‚Äôt get models to, you know, * just * reason. Like, dude, just train it more, and it‚Äôll start reasoning well. This is not the case because it predicts the next token mimicking the training data.\nThe training data also contains text (blogs, papers, etc.) where humans have written detailed explanations on why they have done what they have done. So, naturally (or based on evals, you cheeky vibe-based-eval person), you can expect the model to perform better at solving problems by first spitting out the steps or what it thinks about the problem. It‚Äôs essentially the model giving more context to itself for better next token prediction.\nThis is reffered to as Chain of Thought (CoT). It is one interesting way to get them to reason (or a dumbed down way of actual human reasoning). The models are explicitly begged asked to think out before giving out an answer.\nOpenAI took this to next level by training an LLM to be good at this.\nThis maps to humans as well in the sense that we see a lot of benefits when we think out loud or write down our thoughts. I know that we can reason/think quite well ‚Äúin our brains‚Äù but it‚Äôs better the other way in most cases.\nThis especially applies to writing about some material you just learnt. You identify a lot of gaps in your understanding when you do this and are using your ‚Äútest-time compute‚Äù to understand, make connections and discover new things in your head."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me!",
    "section": "",
    "text": "Salut! Je m‚Äôappelle Suchit! I am a CS engineering sophomore at Ramaiah Institute of Technology at the Silicon Valley of India, Bangalore.\nI have a strong interest in pursuing a research career in the field of Deep Learning and love the notion of humans working alongside AI. I wish to create AI systems to enhance just about any aspect of the human life.\n\n\n\n\nBachelors in Information Science and Engineering | Dec 2022 - Present (expected: 2026)\n\n\n\n\n\n\n\n\n‚Ä¢ Leveraged RAG to extract fields of interest from contract documents achieving 90%+ accuracy.\n‚Ä¢ Utilized LLM-Agents to create multi-turn conversation dataset between E-Commerce customers and support agents with 15,000+ multi-turn conversations.\n‚Ä¢ Created a detailed analysis of distributions of 8+ dataset augmentation techniques used like tone augmentations, language style augmentations and more.\n\n\n\n\n\n\n‚Ä¢ Configured and set up a 5 Nodes Spark cluster on a remote Hetzner server and open sourced the implementation and steps to reproduce. (GitHub link)\n‚Ä¢ Built a robust and errors-tolerant system to process over 28 TB of data from CommonCrawl (6300 WARC files), thoroughly documenting the entire process. (blog link)\n‚Ä¢ Open sourced processed data (315M+ rows) having IPs, server names, script tag src attributes, etc., enriched with geolocation information. (HuggingFace dataset link)\n‚Ä¢ Extracted and performed analysis of JS library usage for 2024 from the Crawl data achieving same results as in Wappalyzer.com. (blog post link)\n\n\n\n\n\n\n\n\n\n\n\n\n \n  \n   \n  \n    \n     X\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "About me!",
    "section": "",
    "text": "Bachelors in Information Science and Engineering | Dec 2022 - Present (expected: 2026)"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "About me!",
    "section": "",
    "text": "‚Ä¢ Leveraged RAG to extract fields of interest from contract documents achieving 90%+ accuracy.\n‚Ä¢ Utilized LLM-Agents to create multi-turn conversation dataset between E-Commerce customers and support agents with 15,000+ multi-turn conversations.\n‚Ä¢ Created a detailed analysis of distributions of 8+ dataset augmentation techniques used like tone augmentations, language style augmentations and more.\n\n\n\n\n\n\n‚Ä¢ Configured and set up a 5 Nodes Spark cluster on a remote Hetzner server and open sourced the implementation and steps to reproduce. (GitHub link)\n‚Ä¢ Built a robust and errors-tolerant system to process over 28 TB of data from CommonCrawl (6300 WARC files), thoroughly documenting the entire process. (blog link)\n‚Ä¢ Open sourced processed data (315M+ rows) having IPs, server names, script tag src attributes, etc., enriched with geolocation information. (HuggingFace dataset link)\n‚Ä¢ Extracted and performed analysis of JS library usage for 2024 from the Crawl data achieving same results as in Wappalyzer.com. (blog post link)"
  },
  {
    "objectID": "index.html#more-about-me",
    "href": "index.html#more-about-me",
    "title": "About me!",
    "section": "More about me",
    "text": "More about me\nI‚Äôm of the tinkering and learning species and my current interests include Machine Learning, reading anything that seems interesting to me (including but not limited to psychology and self-help books, fiction, and half-assing CS books), learning French and playing the Guitar! I am currently taking the fastai course where I am enjoying the top-down learning and iterative approach by the course.\nI blog about what I think is interesting, worth remembering and putting it out there for primarily myself in the hope that it‚Äôs also useful for others. Feel free to DM or raise a PR in case for any edits/suggestions/additions to one of my posts or if you just want to say HI or talk about anything else (or even offer me an internship ;) )!"
  },
  {
    "objectID": "posts/nlp-terms/index.html",
    "href": "posts/nlp-terms/index.html",
    "title": "Intro to NLP",
    "section": "",
    "text": "The world was not ready for something as crazy as ChatGPT. It took the world by storm and also the creators who were earlier going gaga over Crypto, Web3 and ‚Ä¶ blah blah ‚Ä¶ you name it. But did you know that there were plenty of groundbreaking papers/inventions prior to ChatGPT. There was ULMFiT, ‚ÄúAttention is all you need‚Äù (the authors knew what they were talking about üòè), BERT, etc., that introduced some fundamental architectures/techniques that have led to some state-of-the-art models. Now, excuse me if I have missed out on other revolutionary papers; I have listed only those I have come across.\nIn this brief post I will be going over some fundamental terms used in NLP that are not as complex as they sound to be. I will also attempt to put into words my understanding of input (embeddings) to LMs (Language Models) and give a brief overview of how LMs work.\nYou are not required to have extensive knowledge about neural networks and such to understand all of the writing here, although that would help."
  },
  {
    "objectID": "posts/nlp-terms/index.html#introduction",
    "href": "posts/nlp-terms/index.html#introduction",
    "title": "Intro to NLP",
    "section": "",
    "text": "The world was not ready for something as crazy as ChatGPT. It took the world by storm and also the creators who were earlier going gaga over Crypto, Web3 and ‚Ä¶ blah blah ‚Ä¶ you name it. But did you know that there were plenty of groundbreaking papers/inventions prior to ChatGPT. There was ULMFiT, ‚ÄúAttention is all you need‚Äù (the authors knew what they were talking about üòè), BERT, etc., that introduced some fundamental architectures/techniques that have led to some state-of-the-art models. Now, excuse me if I have missed out on other revolutionary papers; I have listed only those I have come across.\nIn this brief post I will be going over some fundamental terms used in NLP that are not as complex as they sound to be. I will also attempt to put into words my understanding of input (embeddings) to LMs (Language Models) and give a brief overview of how LMs work.\nYou are not required to have extensive knowledge about neural networks and such to understand all of the writing here, although that would help."
  },
  {
    "objectID": "posts/nlp-terms/index.html#terms-you-should-know",
    "href": "posts/nlp-terms/index.html#terms-you-should-know",
    "title": "Intro to NLP",
    "section": "Terms you should know",
    "text": "Terms you should know\n\nTokenization\nThe essence of how neural networks work is that they take in a bunch of numerical inputs, find optimal coefficients (which are tweaked during ‚Äútraining‚Äù to get the desired result) to multiply the inputs with to get the result. Models working with images convert the images into tensors or a bunch of numbers and take that as input. But how do LMs take sentences and possibly entire documents as input? They use something called ‚Äòtokenization‚Äô. The sentences and/or words are broken down into smaller chunks. The specifics of this depends on the ‚Äòtokenizer‚Äô used. For example, here‚Äôs how tokenization might look like:\ntokenize(\"A platypus is an ornithorhyncus anatinus.\")\n['‚ñÅA',\n '‚ñÅplatypus',\n '‚ñÅis',\n '‚ñÅan',\n '‚ñÅor',\n 'ni',\n 'tho',\n 'rhynch',\n 'us',\n '‚ñÅan',\n 'at',\n 'inus',\n '.']\nWhat do we do with these ‚Äòtokens‚Äô now? We convert them to something that can be fed into neural networks, i.e., numbers.\n\n\nNumericalization\nNumericalization does exactly what the name suggests, it numericalizes the tokens. Firstly, a list of unique tokens is made which is called the vocab (there you go, a definition inside another definition. I‚Äôm damn smooth). Then, numbers are assigned to words in the vocab. This set of numbers act as input to the LM. More on this here.\n\n\nFine-tuning\nThe concept of fine-tuning was first introduced in the field of Computer Vision. A model (neural network) trained on a large corpus of related data is taken and the last few layers are then trained (keeping the other layers ‚Äúfreezed‚Äù) on the task specific dataset. The pre-trained model will be having low-level representations from the pre-training which helps in the downstream task. Fine-tuning was introduced in NLP through ‚ÄúUniversal Language Model Fine-tuning for Text Classification‚Äù by Jeremy Howard and Sebastian Ruder."
  },
  {
    "objectID": "posts/nlp-terms/index.html#how-do-lms-work",
    "href": "posts/nlp-terms/index.html#how-do-lms-work",
    "title": "Intro to NLP",
    "section": "How do LMs work?",
    "text": "How do LMs work?\nBaseline and/or task-agnostic LMs are trained to predict the ‚Äúnext words‚Äù after the input. That is, the next words are the ‚Äúdependent variables‚Äù, commonly denoted by y. This technique helps the LMs capture the relationship between words and the models can even generate coherent sentences for a few 10s of words (these models by themselves do not work like ChatGPT does just because they can generate text. That‚Äôs a topic of discussion for another day).\n\nInput to LMs (Embeddings)\nIt is tempting to think that these (numericalized) tokens are used as it is as numbers (as was the case with me). This (also embedded below) wonderful video by StatQuest helped me understand how the inputs work for LMs. The essence of the video is that LMs are first trained to learn the embeddings of the words (before any downstream task, like classification). Each word‚Äôs embedding is in the form of a vector in a n-dimensional vector space. More similar the words, closer are their vector representations.\n\n\n\n\n\n\n\nNerdy words alert\n\n\n\n\n\n\nVector: Any quantity that has quantity and direction, in the most basic sense. In this case, a list of numbers which when converted to a arrow-like thing, points in a particular direction and has a particular length.\nn-dimensional space: Know about 2D and 3D space? nD is something similar. 2 or 3 dimensions are not enough to capture word representations and hence ‚Äòn‚Äô (arbritary, or as decided by the practitioner) dimensions are used for the same. Don‚Äôt worry if you can‚Äôt visualize it. No one can ;)\n\n\n\n\nThank you for reading my blog. You can reach out to me through my socials here:\n\nDiscord - ‚Äúlostsquid.‚Äù\nLinkedIn - /in/suchitg04/\n\nI hope to see you soon. Until then üëã"
  },
  {
    "objectID": "posts/diffedit/diffedit-blog.html",
    "href": "posts/diffedit/diffedit-blog.html",
    "title": "DiffEdit Paper Implementation",
    "section": "",
    "text": "This blog delves into the implementation of the DiffEdit paper. For a beginner who had just learned about SD, implementing this was a little difficult with me misunderstanding so many things in the paper. I will point them out and provide a clear explanation of my implementation in this post. I did have a few (or more than a few üôÇ) peeks at other implementations, primarily this and this. They were helpful in correcting some of my misconceptions."
  },
  {
    "objectID": "posts/diffedit/diffedit-blog.html#imports",
    "href": "posts/diffedit/diffedit-blog.html#imports",
    "title": "DiffEdit Paper Implementation",
    "section": "3.1. Imports",
    "text": "3.1. Imports\nPlease excuse the weird cell metadata showing up here. I tried my best to remove them, but couldn‚Äôt.\n::: {#cell-2 .cell _kg_hide-input=‚Äòfalse‚Äô _kg_hide-output=‚Äòfalse‚Äô editable=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2024-05-04T04:34:49.901038Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2024-05-04T04:34:49.900655Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2024-05-04T04:35:14.827704Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2024-05-04T04:35:14.826502Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2024-05-04T04:34:49.901008Z‚Äù}‚Äô slideshow=‚Äò{‚Äúslide_type‚Äù:‚Äú‚Äú}‚Äô tags=‚Äò[]‚Äô execution_count=1}\n!pip install -q --upgrade transformers==4.25.1 diffusers ftfy accelerate\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.3 requires transformers&gt;=4.33.1, but you have transformers 4.25.1 which is incompatible.\n\n:::\n::: {#cell-3 .cell _kg_hide-output=‚Äòfalse‚Äô editable=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2024-05-04T04:35:14.830043Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2024-05-04T04:35:14.829700Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2024-05-04T04:35:33.791921Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2024-05-04T04:35:33.791024Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2024-05-04T04:35:14.830014Z‚Äù}‚Äô slideshow=‚Äò{‚Äúslide_type‚Äù:‚Äú‚Äú}‚Äô tags=‚Äò[]‚Äô execution_count=2}\nfrom tqdm.auto import tqdm\nfrom diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom PIL import Image\nimport torchvision.transforms as tfms\nimport torch\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n2024-05-04 04:35:21.379115: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-04 04:35:21.379220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-04 04:35:21.520888: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nThe cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n\n\nMoving 0 files to the new cache system\n\n\n\n\n:::"
  },
  {
    "objectID": "posts/diffedit/diffedit-blog.html#models-and-helper-functions",
    "href": "posts/diffedit/diffedit-blog.html#models-and-helper-functions",
    "title": "DiffEdit Paper Implementation",
    "section": "3.2. Models and Helper Functions",
    "text": "3.2. Models and Helper Functions\n\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to('cuda')\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\").to('cuda')\nscheduler = LMSDiscreteScheduler(beta_schedule=\"scaled_linear\")\n\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to('cuda')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'visual_projection.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'logit_scale', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'text_projection.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight']\n- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\ndef latents_to_pil(latents):\n    latents = latents / 0.18215\n    with torch.no_grad():\n        images = vae.decode(latents).sample\n        \n    images = (images / 2 + 0.5).clamp(0,1)\n    images = images.detach().cpu().permute(0, 2, 3, 1).numpy()\n    images = (images * 255).round().astype('uint8')\n    pil_images = [Image.fromarray(image) for image in images]\n    return pil_images\n\ndef pil_to_latent(input_image):\n    with torch.no_grad():\n        latent = vae.encode(tfms.ToTensor()(input_image).unsqueeze(0).to('cuda') * 2 - 1)\n    return 0.18215*latent.latent_dist.sample()\n\nIt is worth spending some time here to understand the functions latents_to_pil and pil_to_latent.\nlatents = latents / 0.18215 - the latents are scaled by 0.18215 before being fed to the diffusion model to have approximately unit variance (I don‚Äôt know why it should have unit variance). So, we divide it by that much to get back the original representation of the latent to then convert it into an image. Check out this GitHub issue where one of the creators of Stable Diffusion has answered the question!\nimages = (images / 2 + 0.5).clamp(0,1) - normalizing the images to be in the range [0, 1]\nimages = images.detach().cpu().permute(0, 2, 3, 1).numpy() - pil expects image dims in different order - H,W,C - whereas the VAE gives - C,H,W\nAnd then we scale the values by 255.\nThe pil_to_latent function is pretty straightforward - the image is passed through the VAE, and then scaled.\nWe will define some helper functions to visualize the latents we create and the intermediate steps during diffusion. Before that, Let‚Äôs download an image of a baseball as an example to edit.\n\n!curl --output baseball_bat.jpg 'https://i5.walmartimages.com/asr/569620aa-0bf3-48a9-9b86-5316127419b2_1.bfbc8c8759fe8f2f2c394f394ee66351.jpeg'\nimg = Image.open('baseball_bat.jpg').resize((512, 512)); img\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  107k  100  107k    0     0  1017k      0 --:--:-- --:--:-- --:--:-- 1021k\n\n\n\n\n\n\n\n\n\n\ndef vis_latent(latent, cmap='Greys'):\n    fig, axs = plt.subplots(1, 4, figsize=(10, 16))\n    for i in range(4):\n        axs[i].imshow(latent[0][i].cpu(), cmap=cmap)\n        \ndef accumulate_images(latent, img_list):\n    img_list.append(\n        latents_to_pil(latent)[0]\n    )\n    \n# Code taken from https://github.com/fastai/diffusion-nbs/blob/master/stable_diffusion.ipynb\ndef image_grid(imgs):\n    w,h = imgs[0].size\n    cols = len(imgs) if len(imgs) &lt; 10 else 10\n    rows = int(len(imgs) / cols) + 1\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols * w, i//cols * h))\n        \n    return grid\n\nLet‚Äôs try out the vis_latent function, and the latent and PIL conversion functions.\n\nl = pil_to_latent(img); vis_latent(l)\n\n\n\n\n\n\n\n\n\ni = latents_to_pil(l); i[0]\n\n\n\n\n\n\n\n\nget_emb_from_prompt takes a prompt, tokenizes it and gives its embeddings.\n\ndef get_emb_from_prompt(prompt):\n    tokens = tokenizer(prompt, max_length=tokenizer.model_max_length, padding='max_length', truncation=True, return_tensors='pt').to('cuda')\n    with torch.no_grad():\n        text_emb = text_encoder(tokens.input_ids)[0]\n    return text_emb"
  },
  {
    "objectID": "posts/diffedit/diffedit-blog.html#diffedit",
    "href": "posts/diffedit/diffedit-blog.html#diffedit",
    "title": "DiffEdit Paper Implementation",
    "section": "3.3. DiffEdit",
    "text": "3.3. DiffEdit\nWe will be closely following what‚Äôs mentioned in the DiffEdit paper for this implementation, primarily sections 3.1 and 3.2.\nHere‚Äôs a screenshot from section 3.1 that gives an overview of the DiffEdit framework.\n\n\n3.3.1. Step 1: Computing Editing Mask\nAdd noise to the input image and denoise it: once conditioned on the query text, and once conditioned on a reference text (or unconditionally). We derive a mask based on the difference in the denoising results.\nThe model, when conditioned on the query text, gives a noise output that covers everything that isn‚Äôt what is described by the query text. So, if we were to set query_prompt to ‚Äúpixelated minecraft stick‚Äù, the noise output would cover the baseball bat. When it is conditioned on the reference text, that describes the original image, the noise output covers everything except the baseball bat that the reference text describes. When we take the difference of these two noise latents, we get an area that covers only the part which is to be edited.\nLet‚Äôs see this in action.\n\n\n\n\n\n\nNote\n\n\n\n\n\nA mistake I made here is that I mistook the the phrase ‚Äúdenoise it‚Äù to be the entire diffusion denoising process for inference_steps number of steps. This cost me a week‚Äôs time and I gave in to peeking at another implementation only to find out that I was overthinking :‚Äô)\n\n\n\n\ntorch.manual_seed(42)\n\n# denoising conditioned on the query text\nquery_emb = get_emb_from_prompt(\"pixelated minecraft stick\")\nim_latent = pil_to_latent(img)\nguidance = 7\ninference_steps = 50\n# timestep_index is set to 25 because we are adding 50% noise like in the paper\ntimestep_index = 25\n\n# following is a regular img2img pipeline that denoises for a single step\nscheduler.set_timesteps(inference_steps)\nnoise_timestep = scheduler.timesteps[-timestep_index]\nuncond_emb = get_emb_from_prompt(\"\")\ntext_embeddings = torch.cat([uncond_emb, query_emb])\n\nnoise = torch.randn_like(im_latent, device='cuda')\nnoisy_latent = scheduler.add_noise(im_latent, noise, timesteps=torch.tensor([noise_timestep]))\nnoisy_latent = noisy_latent * scheduler.sigmas[timestep_index]\n\nlatent_model_input = torch.cat([noisy_latent] * 2)\nlatent_model_input = scheduler.scale_model_input(latent_model_input, noise_timestep)\n\nwith torch.no_grad():\n    pred_uncond, pred_cond = unet(latent_model_input, noise_timestep, encoder_hidden_states=text_embeddings).sample\n    \nquery_noise_pred = pred_uncond + guidance * (pred_cond - pred_uncond)\nquery_noise_pred = query_noise_pred[None].detach()\n\n\nvis_latent(query_noise_pred)\n\n\n\n\n\n\n\n\n\nlatents_to_pil(query_noise_pred.cuda())[0]\n\n\n\n\n\n\n\n\nWe can put that in a function and compute the difference between the noise conditioned on the query prompt and the noise conditioned on the reference prompt.\n\ndef predict_noise(im_latent, prompt_emb, uncond_emb, seed=42, strength=0.5):\n    torch.manual_seed(seed)\n    guidance = 7\n    inference_steps = 50\n    timestep_index = int(inference_steps * strength)\n\n    # following is a regular img2img pipeline that denoises for a single step\n    scheduler.set_timesteps(inference_steps)\n    noise_timestep = scheduler.timesteps[-timestep_index]\n    text_embeddings = torch.cat([uncond_emb, prompt_emb])\n\n    noise = torch.randn_like(im_latent, device='cuda')\n    noisy_latent = scheduler.add_noise(im_latent, noise, timesteps=torch.tensor([noise_timestep]))\n    noisy_latent = noisy_latent * scheduler.sigmas[timestep_index]\n\n    latent_model_input = torch.cat([noisy_latent] * 2)\n    latent_model_input = scheduler.scale_model_input(latent_model_input, noise_timestep)\n\n    with torch.no_grad():\n        pred_uncond, pred_cond = unet(latent_model_input, noise_timestep, encoder_hidden_states=text_embeddings).sample\n\n    noise_pred = pred_uncond + guidance * (pred_cond - pred_uncond)\n    return noise_pred[None]\n\n\nref_emb = get_emb_from_prompt(\"baseball bat\")\nref_noise_pred = predict_noise(im_latent, ref_emb, uncond_emb)\n\n\nvis_latent(ref_noise_pred)\n\n\n\n\n\n\n\n\n\nvis_latent((ref_noise_pred - query_noise_pred).abs())\n\n\n\n\n\n\n\n\nWe have to repeat the process for a set of n input noises and average the differences to get a more prominent difference.\nThe paper uses n=10, but in my experiments n=15 gave slightly better noise differences.\n\n\n\n\n\n\nNote\n\n\n\n\n\nHere, note that we need to use a set of n input noises generated from different seeds.\n\n\n\n\n# noise_r, noise_q, diff_noise = [torch.zeros_like(im_latent) for _ in range(3)]\nnoise_diff = torch.zeros_like(im_latent)\n\nfor i in tqdm(range(15)):\n    seed = i * 4\n    noise_r = predict_noise(im_latent, ref_emb, uncond_emb, seed, 0.6)\n    noise_q = predict_noise(im_latent, query_emb, uncond_emb, seed, 0.6)\n    noise_diff += (noise_r - noise_q).abs()\n    \n# averaging\nnoise_diff /= 15\n\n\n\n\n\nvis_latent(noise_diff)\n\n\n\n\n\n\n\n\n\nlatents_to_pil(noise_diff)[0]\n\n\n\n\n\n\n\n\n\ndef compute_diff(im_latent, query_emb, ref_emb, uncond_emb, n=15, seed=42):\n    noise_diff = torch.zeros(1, unet.config.in_channels, 512//8, 512//8).to('cuda')\n\n    print(\"Computing average noise estimate difference ...\")\n    for i in tqdm(range(n)):\n        seed *= i # arbitrary operation to get a seed\n        noise_r = predict_noise(im_latent, ref_emb, uncond_emb, seed, 0.5)\n        noise_q = predict_noise(im_latent, query_emb, uncond_emb, seed, 0.5)\n        noise_diff += (noise_r - noise_q).abs()\n    \n    return noise_diff / n\n\nThe noise difference does not look all that interesting until we binarize it with a threshold.\n\nnoise_diff.shape\n\ntorch.Size([1, 4, 64, 64])\n\n\nWe first take the mean of the 4 channels in the latent and normalize it.\n\nmask = noise_diff\nmask = mask.squeeze().mean(0)\nmask = (mask - mask.min()) / mask.max()\n\n\nmask.min(), mask.max()\n\n(tensor(0., device='cuda:0'), tensor(0.9782, device='cuda:0'))\n\n\nWe can then pass it through a threshold to get the desired mask. The threshold value is determined by trial and error, but in most cases I found that 0.1 was a good default in this implementation.\n\nbin_mask = mask &gt; 0.1\nplt.imshow(bin_mask.cpu())\n\n\n\n\n\n\n\n\nThe mask looks good, but we can do even better by eliminating the jagged edges and abnormal holes at random places. We do that by blurring the noise_diff latent which also gives us a slightly larger region to edit.\nHere, I chose the kernel and sigma values by simply fiddling around with them to see what worked best. Refer to the documentation to learn more.\n\nblurrer = tfms.GaussianBlur((9,9), sigma=1.5)\nvis_latent(blurrer(noise_diff)); latents_to_pil(blurrer(noise_diff))[0]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we re-compute the mask with the blurred latent.\n\nmask = blurrer(noise_diff)\nmask = mask.squeeze().mean(0)\nmask = (mask - mask.min()) / mask.max()\n\nmask.min(), mask.max()\n\nbin_mask = (mask &gt; 0.09).to(torch.float32)\nplt.imshow(bin_mask.cpu())\n\n\n\n\n\n\n\n\nThat looks much better!\n\ndef compute_bin_mask(noise_diff, mask_threshold=0.1, blurrer=None):\n    if blurrer:\n        mask = blurrer(noise_diff)\n    else:\n        mask = noise_diff\n    mask = mask.squeeze().mean(0)\n    mask = (mask - mask.min()) / mask.max()\n    bin_mask = (mask &gt; mask_threshold).to(float)\n    # to avoid \"mat1 and mat2 are of different dtypes\" error in the last step\n    return bin_mask.to(torch.float32)\n\nCool! Everything up to here works perfectly! Let‚Äôs remind ourselves of the remaining steps:\n\nadd noise to the image to an ‚Äúextent‚Äù (referred to as the encoding ratio in the paper)\nuse the img2img denoising pipeline but after each step replace the non-masked area (the purple colored area in the above image) with the image from the previous timestep.\n\nThat brings us to the next step ‚Ä¶\n\n\n3.3.2. Step 2: Encoding\nLet us understand what this means by referring to the timesteps vs sigmas graph.\n\n# Code taken from https://github.com/fastai/diffusion-nbs/blob/master/Stable%20Diffusion%20Deep%20Dive.ipynb\nplt.plot(scheduler.sigmas)\nplt.title('Noise Schedule')\nplt.xlabel('Sampling step')\nplt.ylabel('sigma')\nplt.show()\n\n\n\n\n\n\n\n\nIn a typical txt2img pipeline, the image is initially at the 0th timestep. We try to get the image at the next timestep ((n=n+1)th) timestep at each denoising step and we perform this process 50 times (considering num_inference_steps = 50).\nHere, the original image that is to be edited is considered to be at the 50th timestep. We add noise to it corresponding to timestep r. This is also called the encoding ratio in the paper. The choice of the encoding ratio influences the strength of the edit. Please refer to the appendix A.4. in the paper that shows the impact of the encoding ratio.\n\n\n\n\n\n\nNote\n\n\n\nThe paper describes the encoding step as follows: ‚ÄúWe encode the input image x0 in the implicit latent space at timestep r with the DDIM encoding function \\(E_r\\). This is done with the unconditional model, i.e.¬†using conditioning text ‚àÖ, so no text input is used for this step.‚Äù I didn‚Äôt really understand how the image can be encoded using the conditioning text ‚àÖ (‚Äú‚Äú). I think they just mean Gaussian noise, but I‚Äôm not entirely sure.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nThe paper also refers to r as both the encoding ratio and the noise timestep. I think this is a mistake in usage (or I might not be understanding it right ¬Ø\\_(„ÉÑ)_/¬Ø).\n\n\n\n\n# According to my understanding the number of inference steps (timesteps) shouldn't matter much\nscheduler.set_timesteps(100)\nencoding_ratio = 0.7 # strength of edit. stronger edit =&gt; closer to 1\nstrength = 1 - encoding_ratio\ntimestep_index = round(strength * scheduler.timesteps.shape[0])\nnoise_timestep = scheduler.timesteps[timestep_index]\ntimesteps = scheduler.timesteps[timestep_index:]\n\nnoise = torch.randn_like(im_latent, device='cuda')\nnoisy_latent = scheduler.add_noise(im_latent, noise, timesteps=torch.tensor([noise_timestep]))\n\nvis_latent(noisy_latent)\n\n\n\n\n\n\n\n\n\nlatents_to_pil(noisy_latent)[0]\n\n\n\n\n\n\n\n\nThat is it for this step. We now move on to the next and final step ‚Ä¶\n\n\n3.3.3. Step 3: Decoding with Mask Guidance\nAt each timestep, we do the exact same thing as in a img2img pipeline ‚Äî guidance and computing the latent at r-1th timestep ‚Äî and also apply the mask. We do this denoising process until the 0th timestep.\nThe formula for applying the mask given in the paper is:\n\\[\\begin{equation}\n\\tilde{y}_t = My_t + (1-M)x_t\\\\\n\\end{equation}\\] \\[\\begin{align*}\n\\text{where:}\\quad\nM &= \\text{binarized mask}\\\\\ny_t &= \\text{the latent at timestep } t-1 \\text{ computed from the predicted noise}\\\\\nx_t &= \\text{original image latent noised at timestep } t-1\n\\end{align*}\\]\n\\(\\tilde{y_t}\\) is then used as the input to the UNet in the next step.\n\ntext_embeddings = torch.cat([uncond_emb, query_emb])\nguidance = 12\n\nfor i,t in tqdm(enumerate(timesteps), total=len(timesteps)):\n    latent_model_input = torch.cat([noisy_latent] * 2)\n    latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n    with torch.no_grad():\n        pred_uncond, pred_cond = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n    noise_pred = pred_uncond + guidance * (pred_cond - pred_uncond)\n    if (i != timesteps.shape[0]):\n        x_t = scheduler.add_noise(im_latent, torch.randn_like(im_latent, device='cuda'), torch.tensor([t]))\n    else:\n        x_t = im_latent\n        \n    y_t = scheduler.step(noise_pred, t, noisy_latent).prev_sample\n    noisy_latent = bin_mask * y_t + ((1 - bin_mask) * x_t)\n\n\n\n\n\nlatents_to_pil(noisy_latent)[0]\n\n\n\n\n\n\n\n\nWell, that looks like a cooked fish stuck on top of the stick üòÇ and it works!\n\ndef txt2edited(im_latent, query_emb, uncond_emb, bin_mask, encoding_ratio=0.6, guidance_scale=7, callback=None, callback_steps=5):\n    scheduler.set_timesteps(100)\n    strength = 1 - encoding_ratio\n    timestep_index = round(strength * scheduler.timesteps.shape[0])\n    noise_timestep = scheduler.timesteps[timestep_index]\n    timesteps = scheduler.timesteps[timestep_index:]\n\n    noise = torch.randn_like(im_latent, device='cuda')\n    noisy_latent = scheduler.add_noise(im_latent, noise, timesteps=torch.tensor([noise_timestep]))\n    \n    text_embeddings = torch.cat([uncond_emb, query_emb])\n    inter_imgs = []\n    print(\"Editing image ...\")\n    for i,t in tqdm(enumerate(timesteps), total=len(timesteps)):\n        latent_model_input = torch.cat([noisy_latent] * 2)\n        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n\n        with torch.no_grad():\n            pred_uncond, pred_cond = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n        noise_pred = pred_uncond + guidance_scale * (pred_cond - pred_uncond)\n        if (i != timesteps.shape[0]):\n            x_t = scheduler.add_noise(im_latent, torch.randn_like(im_latent, device='cuda'), torch.tensor([t]))\n        else:\n            x_t = im_latent\n\n        y_t = scheduler.step(noise_pred, t, noisy_latent).prev_sample\n        noisy_latent = bin_mask * y_t + ((1 - bin_mask) * x_t)\n        vis_lat = bin_mask * y_t + ((1 - bin_mask) * im_latent)\n        \n        if callback and i % callback_steps == 0:\n            callback(vis_lat, inter_imgs)\n            \n    return (noisy_latent, inter_imgs) if callback else (noisy_latent, None)\n\n\ntmp,_ = txt2edited(im_latent, query_emb, uncond_emb, bin_mask, 0.7, 12); latents_to_pil(tmp)[0]\n\nEditing image ...\n\n\n\n\n\n\n\n\n\n\n\n\nWe can now put this all into a single function that takes the following as inputs:\n\nPIL image to be edited\ndescription of the image (default: ‚Äú‚Äú)\nquery prompt\nnum_diffs (default: 15, the number of noise differences to average)\nmask_threshold (default: 0.1)\nencoding_ratio (default: 0.7)\nguidance_scale (default: 12, to be used in the txt2edited function)\n\nand gives out the edited image.\n\ndef diffEdit(img, query_prompt, ref_prompt=\"\", num_diffs=15, seed=42, mask_threshold=0.1, blurrer=None, encoding_ratio=0.7, guidance_scale=12, callback=None, callback_steps=5):\n    im_latent = pil_to_latent(img)\n    query_emb = get_emb_from_prompt(query_prompt)\n    ref_emb = get_emb_from_prompt(ref_prompt)\n    noise_diff = compute_diff(im_latent, query_emb, ref_emb, uncond_emb, num_diffs, seed)\n    bin_mask = compute_bin_mask(noise_diff, mask_threshold, blurrer)\n    edited_latent, inter_imgs = txt2edited(im_latent, query_emb, uncond_emb, bin_mask, encoding_ratio, guidance_scale, callback, callback_steps)\n    return latents_to_pil(edited_latent)[0], inter_imgs"
  },
  {
    "objectID": "posts/mnist/index.html",
    "href": "posts/mnist/index.html",
    "title": "Classifying handwritten digits (THE MNIST!)",
    "section": "",
    "text": "From MNIST being the first Machine Learning dataset I heard about to building a model for it, I can surely say Machine Learning is a fascinating subject to study! Building a ‚Äúmodel‚Äù is more than just getting a model and training it. It involves:\n\nGetting the data\nPreprocessing the data, i.e., cleaning it, converting it into a format that the model can understand, etc.\nCreating the training, validation, and test (test data is not considered in this case) split\nTraining the model\nFine-tuning hyper-parameters based on the inferences gotten from the accuracy and other metrics over the validation dataset, and then improving the model (not done in this case)\n\nGiven above is the rough process of building a Machine Learning project. This project maintains a medium level of abstraction and doesn‚Äôt entirely utilize high-level functions but doesn‚Äôt go deep into the low-level implementations either. I aim to maintain an understandable and yet not-so-abstracted level of coding throughout. Let‚Äôs get started!\n\n\n\n\n\n\nImportant\n\n\n\nIf you are referring to this post as a guide, then you are expected to have the following pre-requisites to fully and deeply understand what‚Äôs going on:\n\nBasics of Python\nImporting modules, methods and calling them in Python\nA fundamental idea of how a typical Linear Regression model works (knowledge of simple neural networks recommended but not necessary)\nWillingness and the ability to google and read through documentation ;)\n\n\n\n\n\n\n\n\n\nSome useful resources\n\n\n\n\nHow does a neural net actually work?\nfastai MNIST chapter\nMy MNIST (2 digits) notebook (linked again later below)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf I have trouble understanding a piece of code written by others, here‚Äôs what I do:\n\nTry to speak out loudly and explain the code to myself (rubber ducking).\nSearch the official docuementation or stack overflow and understand through examples.\nIf the above two approaches don‚Äôt work, then ask ChatGPT to explain the code to me. This step works no matter what!\n\n\n\n\n# Importing the necessary libraries and modules\n\nfrom fastai.vision.all import *\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import SubsetRandomSampler, random_split\nfrom sklearn.model_selection import train_test_split\n\nmatplotlib.rc('image', cmap='Greys')"
  },
  {
    "objectID": "posts/mnist/index.html#introduction",
    "href": "posts/mnist/index.html#introduction",
    "title": "Classifying handwritten digits (THE MNIST!)",
    "section": "",
    "text": "From MNIST being the first Machine Learning dataset I heard about to building a model for it, I can surely say Machine Learning is a fascinating subject to study! Building a ‚Äúmodel‚Äù is more than just getting a model and training it. It involves:\n\nGetting the data\nPreprocessing the data, i.e., cleaning it, converting it into a format that the model can understand, etc.\nCreating the training, validation, and test (test data is not considered in this case) split\nTraining the model\nFine-tuning hyper-parameters based on the inferences gotten from the accuracy and other metrics over the validation dataset, and then improving the model (not done in this case)\n\nGiven above is the rough process of building a Machine Learning project. This project maintains a medium level of abstraction and doesn‚Äôt entirely utilize high-level functions but doesn‚Äôt go deep into the low-level implementations either. I aim to maintain an understandable and yet not-so-abstracted level of coding throughout. Let‚Äôs get started!\n\n\n\n\n\n\nImportant\n\n\n\nIf you are referring to this post as a guide, then you are expected to have the following pre-requisites to fully and deeply understand what‚Äôs going on:\n\nBasics of Python\nImporting modules, methods and calling them in Python\nA fundamental idea of how a typical Linear Regression model works (knowledge of simple neural networks recommended but not necessary)\nWillingness and the ability to google and read through documentation ;)\n\n\n\n\n\n\n\n\n\nSome useful resources\n\n\n\n\nHow does a neural net actually work?\nfastai MNIST chapter\nMy MNIST (2 digits) notebook (linked again later below)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf I have trouble understanding a piece of code written by others, here‚Äôs what I do:\n\nTry to speak out loudly and explain the code to myself (rubber ducking).\nSearch the official docuementation or stack overflow and understand through examples.\nIf the above two approaches don‚Äôt work, then ask ChatGPT to explain the code to me. This step works no matter what!\n\n\n\n\n# Importing the necessary libraries and modules\n\nfrom fastai.vision.all import *\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import SubsetRandomSampler, random_split\nfrom sklearn.model_selection import train_test_split\n\nmatplotlib.rc('image', cmap='Greys')"
  },
  {
    "objectID": "posts/mnist/index.html#downloading-the-dataset",
    "href": "posts/mnist/index.html#downloading-the-dataset",
    "title": "Classifying handwritten digits (THE MNIST!)",
    "section": "1. Downloading the Dataset",
    "text": "1. Downloading the Dataset\nPyTorch‚Äôs torchvision has a module named datasets that has some some popular datasets available to download and store it in a directory specified by you. If the dataset already exists, then you just need to give it the path to the dataset and it‚Äôll skip downloading it. Click here to know more.\n\ndset = datasets.MNIST(\n    root='/home/suchitg/mnist/dset/', # Creates a folder named 'dset' if it's not already created\n    train=True, # Specifies what set of data to download (train or test)\n    transform=transforms.Compose( # Applies image transformations\n        [transforms.ToTensor(),\n         transforms.RandomRotation(degrees=30)\n        ]\n    ),\n    download=True\n)\n\nprint(dset)\nprint(dset.data.size())\n\nDataset MNIST\n    Number of datapoints: 60000\n    Root location: /home/suchitg/mnist/dset/\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n               RandomRotation(degrees=[-30.0, 30.0], interpolation=nearest, expand=False, fill=0)\n           )\ntorch.Size([60000, 28, 28])\n\n\ntorchvision.transforms.RandomRotation is used to augment the image being loaded into a dataloader by randomly rotating the images about 30¬∞. The reason for selecting this transform was because different handwritings write different digits at varying angles. So, the transformation accomodates for this nuance.\n\ntype(dset)\n\ntorchvision.datasets.mnist.MNIST"
  },
  {
    "objectID": "posts/mnist/index.html#train-val-split-and-normalization",
    "href": "posts/mnist/index.html#train-val-split-and-normalization",
    "title": "Classifying handwritten digits (THE MNIST!)",
    "section": "2. Train-Val split and Normalization",
    "text": "2. Train-Val split and Normalization\nHere, we use sklearn.model_selection.train_test_split to split the data into training and validation datasets. But what is a validation dataset? When we train the model, we improve it by looking at the loss calculated on training set itself. But the accuracy (in this case, how many digits the model gets right) is calculated by inputting the images from the validation dataset into the model. That way, we can know how well the model performs on images that it hasn‚Äôt seen before. If the validation is significantly higher than the training loss, then that indicates a case of overfitting.\nIt is also worth noting that, here, we create a stratified train-val split. What that means is that the number of images from each class is roughly equal. This ensures that the model will be equally good in classifying all digits.\n\ntrain_x, valid_x, train_y, valid_y = train_test_split(dset.data, dset.targets, test_size=0.2, stratify=dset.targets)\n\nLet us now see an example of how a digit looks in our dataset.\n\nplt.imshow(train_x.numpy()[99]), train_y[99]\n\n\n\n\n\n\n\n\nEach image train_x[i] has pixel values between 0 and 255 as shown below.\n\ntrain_x[0].min(), train_x[0].max()\n\n(tensor(0, dtype=torch.uint8), tensor(255, dtype=torch.uint8))\n\n\nNow, we normalize the data. Normalization is not necessarily required here, but we are adding it anyways because it offers a host of benefits. For starters, it helps the model converge, i.e., find a minima of the loss function, faster.\n\ntrain_x = train_x.view(-1, 28*28).float() / 255\nvalid_x = valid_x.view(-1, 28*28).float() / 255\ntrain_y = train_y.unsqueeze(1)\nvalid_y = valid_y.unsqueeze(1)\ntrain_x.shape, valid_x.shape, train_y.shape, valid_y.shape\n\n(torch.Size([48000, 784]),\n torch.Size([12000, 784]),\n torch.Size([48000, 1]),\n torch.Size([12000, 1]))\n\n\nNotice how we are unpacking the target variables along the 2nd dimension or as a column vector, so to speak. You‚Äôll learn why as you read through further.\nLet‚Äôs have a look at an example from our normalized data.\n\ntrain_x[0].min(), train_x[0].max()\n\n(tensor(0.), tensor(1.))\n\n\nThe pixel values are now between 0 and 1!\nLook at how preparing and preprocessing the data is as crucial as building a model for it!"
  },
  {
    "objectID": "posts/mnist/index.html#loading-the-dataset-for-training-using-dataloader",
    "href": "posts/mnist/index.html#loading-the-dataset-for-training-using-dataloader",
    "title": "Classifying handwritten digits (THE MNIST!)",
    "section": "3. Loading the Dataset for Training using DataLoader",
    "text": "3. Loading the Dataset for Training using DataLoader\nBefore we load the data into a DataLoader, we first have to prepare the data in the proper format for it. DataLoader, as per the documentation takes map-style and iterable-style datasets. So we supply it with one.\n\ntrain_dset = list(zip(train_x, train_y))\nvalid_dset = list(zip(valid_x, valid_y))\n\ntype(valid_dset), type(valid_dset[0])\n\n(list, tuple)\n\n\n\ntrain_dl = DataLoader(train_dset, batch_size=256, shuffle=True)\nvalid_dl = DataLoader(valid_dset, batch_size=256, shuffle=False)\ndls = DataLoaders(train_dl, valid_dl) # fast.ai wrapper that encapsulates train_dl and valid_dl"
  },
  {
    "objectID": "posts/mnist/index.html#creating-a-loss-function",
    "href": "posts/mnist/index.html#creating-a-loss-function",
    "title": "Classifying handwritten digits (THE MNIST!)",
    "section": "4. Creating a Loss Function",
    "text": "4. Creating a Loss Function\nWe now have to create a loss function that suits our dataset. I spent more time figuring out and getting the loss function to work than on the other parts! I‚Äôll put both my original code and the optimized version (by ChatGPT). And no, I will not go through the code because well, my brain‚Äôs already fried from writing it üòÇ. So I suggest you use ChatGPT to explain the code to you or even better, try to figure it out yourselves! It will test your understanding of how the data is structured, and you‚Äôll also have to look at what the model spits out.\n\n# def mse_loss(preds, targets):\n#     preds = preds.sigmoid()\n#     loss = []\n    \n#     for pred, target in zip(preds, targets):\n#         for p in range(len(pred)):\n#             if p == target:\n#                 loss.insert(p, (1 - pred[p])**2)\n#             else:\n#                 loss.insert(p, pred[p]**2)\n#             loss[p] = loss[p].mean().view(1)\n\n#     loss = torch.cat(loss)\n#     return loss.mean()\n\n# Optimized code by ChatGPT\ndef mse_loss(preds, targets):\n    preds = preds.sigmoid()\n    loss = torch.zeros_like(preds)\n\n    for i, target in enumerate(targets):\n        loss[i, target] = (1 - preds[i, target]) ** 2\n        loss[i] += preds[i] ** 2\n\n    return loss.mean()\n\n\n\n\n\n\n\nNote\n\n\n\nI‚Äôm using a custom loss function just to demonstrate the performance difference between this loss function (mean squared error) and cross entropy loss (provided by fastai)."
  },
  {
    "objectID": "posts/mnist/index.html#creating-a-neural-network-and-training-it",
    "href": "posts/mnist/index.html#creating-a-neural-network-and-training-it",
    "title": "Classifying handwritten digits (THE MNIST!)",
    "section": "5. Creating a Neural Network and Training it",
    "text": "5. Creating a Neural Network and Training it\nI have chosen the number of neurons and in each layer without any particular reason. So, you can play around with that and see if a lower number works for you as that would bring down the training time. The last layer has to have 10 neurons with output for each digit and that can‚Äôt be changed.\nHere, nn.ReLU is used to add non-linearity to the model. Otherwise, the model would still be a linear model no matter how many nn.Linear you add.\n\nn_net = nn.Sequential(\n    nn.Linear(28*28, 250),\n    nn.ReLU(),\n    nn.Linear(250, 50),\n    nn.ReLU(),\n    nn.Linear(50, 10)\n)\n\n‚Äôtis time Ladies and Gentlemen! ‚Äôtis time to train the model! and fret about how slow MSE is or at least how slow my implementation is\nfastai provides a Learner class that groups together a model, a loss function and a DataLoader object to handle the training. If you want to have a look at the implementation of the entire training process (with minimal to no usage of high-level library functions), then refer to this implementation on a sample 2 digits MNIST dataset.\n\nlearn_mse = Learner(dls, n_net, loss_func=mse_loss, metrics=accuracy)\nlearn_mse.fit(8)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.071596\n0.063623\n0.857333\n01:13\n\n\n1\n0.058387\n0.057266\n0.916000\n01:19\n\n\n2\n0.055814\n0.055529\n0.931000\n01:16\n\n\n3\n0.054634\n0.054546\n0.940333\n01:16\n\n\n4\n0.053810\n0.053983\n0.948583\n01:16\n\n\n5\n0.053269\n0.053475\n0.954667\n01:14\n\n\n6\n0.052900\n0.053110\n0.957667\n01:11\n\n\n7\n0.052528\n0.052865\n0.961500\n01:12\n\n\n\n\n\n\nlearn_ce = Learner(dls, n_net, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn_ce.fit(8)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.328054\n0.274868\n0.922333\n00:53\n\n\n1\n0.207560\n0.196696\n0.945000\n00:51\n\n\n2\n0.148225\n0.152403\n0.956667\n00:51\n\n\n3\n0.113479\n0.128592\n0.962333\n00:53\n\n\n4\n0.088385\n0.113987\n0.967000\n00:55\n\n\n5\n0.073747\n0.105821\n0.968583\n00:53\n\n\n6\n0.061614\n0.099214\n0.971083\n00:51\n\n\n7\n0.048661\n0.093742\n0.971500\n00:51\n\n\n\n\n\nfastai‚Äôs CrossEntropyLossFlat() is clearly faster than our custom loss function and also seems to be giving slightly higher accuracy.\nNow, let‚Äôs use our model to make predictions on different digits.\n\nlen(first(train_dl)[0][1]), len(first(train_dl)[1][1])\n\n(784, 1)\n\n\n\ntest_item = first(train_dl) # Gets a batch from the training dataloader\ntorch.softmax(n_net(test_item[0][3]), 0), test_item[1][3] # Note: Do not directly substitute first(dl)[][] in place of test_item.\n# Because first() gives different batches each time it is run\n\n(tensor([1.6710e-05, 3.6341e-01, 5.4452e-01, 1.9914e-02, 8.2348e-05, 8.3202e-05,\n         3.6827e-05, 1.6098e-02, 5.5500e-02, 3.3604e-04],\n        grad_fn=&lt;SoftmaxBackward0&gt;),\n tensor([2]))\n\n\nRun the above code cell as many number of times you want because it gives a different digit as input each time. Well! Looks like we have accomplished what we have set out to do, i.e., classify handwritten digits!\nIf you have followed this as a guide, then congrats on reading through everything! To challenge yourself further you can join one of these Kaggle competitions and make a submission. But note that your ‚Äúdata pipeline‚Äù (yes, I feel like a God using this word) for the competition has to be different than what we have done here because you‚Äôll be given a CSV file to work with.\nHere‚Äôs a wonderful post that goes into the bits and pieces of how to implement what we have done here from scratch.\nThank you for reading my blog. You can reach out to me through my socials here:\n\nDiscord - ‚Äúlostsquid.‚Äù\nLinkedIn - /in/suchitg04/\n\nI hope to see you soon. Until then üëã"
  },
  {
    "objectID": "posts/data-cleaning/index.html",
    "href": "posts/data-cleaning/index.html",
    "title": "Data Cleaning and Augmentation",
    "section": "",
    "text": "It is a pretty intuitive thought that data is cleaned before training a model so that the model achieves good accuracy. What‚Äôs data cleaning in the first place? Naively, it is removing unrelated data that might have creeped through or removing data unrelated to what the model needs in training. I believe you get my point.\nLet‚Äôs take an example of a model that you are building to, say, classify between the faces of Elon Musk and Mark Zuckerberg. You‚Äôll probably download the images for your dataset from Google or Bing or any such search engines. With Mark rumored to be an alien and people taking a huge liking to the ‚Äúfemale version‚Äù of Elon, it is very certain that your dataset will contain a few such memes.\n\n\nAlien Zuckerberg\n\n\n\nElona Musk\n\nNow, sifting through the data in the hopes of finding such memes can be tedious. How about we let the model decide what images are faulty? Here‚Äôs how it works. You quickly train a model, the ‚Äúlosses‚Äù and accuracies get recorded, and then you pop up some images in decreasing order of ‚Äúloss‚Äù and/or accuracy (there are some tools already that can do that). There you go, the model now helped you find some black sheeps in your data that it found difficult to classify and/or has low confidence about."
  },
  {
    "objectID": "posts/data-cleaning/index.html#data-cleaning",
    "href": "posts/data-cleaning/index.html#data-cleaning",
    "title": "Data Cleaning and Augmentation",
    "section": "",
    "text": "It is a pretty intuitive thought that data is cleaned before training a model so that the model achieves good accuracy. What‚Äôs data cleaning in the first place? Naively, it is removing unrelated data that might have creeped through or removing data unrelated to what the model needs in training. I believe you get my point.\nLet‚Äôs take an example of a model that you are building to, say, classify between the faces of Elon Musk and Mark Zuckerberg. You‚Äôll probably download the images for your dataset from Google or Bing or any such search engines. With Mark rumored to be an alien and people taking a huge liking to the ‚Äúfemale version‚Äù of Elon, it is very certain that your dataset will contain a few such memes.\n\n\nAlien Zuckerberg\n\n\n\nElona Musk\n\nNow, sifting through the data in the hopes of finding such memes can be tedious. How about we let the model decide what images are faulty? Here‚Äôs how it works. You quickly train a model, the ‚Äúlosses‚Äù and accuracies get recorded, and then you pop up some images in decreasing order of ‚Äúloss‚Äù and/or accuracy (there are some tools already that can do that). There you go, the model now helped you find some black sheeps in your data that it found difficult to classify and/or has low confidence about."
  },
  {
    "objectID": "posts/data-cleaning/index.html#data-augmentation",
    "href": "posts/data-cleaning/index.html#data-augmentation",
    "title": "Data Cleaning and Augmentation",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nThe world‚Äôs gone dystopian and governments are crumbling, and Elon and Mark have decided to collaborate and take advantage of this calamity. You are the hero in this situation.\nYou observe that there is a lot of movement in and out of the abandoned Twitter office (Elon‚Äôs selling tickets to Mars to the elites lol). Looks like someone‚Äôs having some in-person meetings ü§®. Now you install a camera along with a image/video recognition model to alert you whenever Elon or Mark comes and goes in/out of the office, but you don‚Äôt get even a single trigger for days!\nYou scratch your head thinking about what could be wrong for hours until you realise that you had trained the model using just headshots and few such ‚Äúpresentable‚Äù images that you scavenged from what‚Äôs left of the internet, but you have installed your camera in such a place that it does not get such good images.\nWhat‚Äôs the solution you may ask? This is where Data Augmentation comes into the picture. You use this technique and apply certain effects on the images like cropping of random parts of the image, applying different colour filters, distorting the image, etc. Not only does this expand the dataset, but it also enables the model to better understand the object it is learning (Elon and Mark, in this case).\nHere‚Äôs an example of what data augmentation does: \nData augmentation is particularly useful when you have a small dataset. It helps bring some variance and helps avoid overfitting if done properly. Give this interesting article a read: Regularization Effect of Data Augmentation.\nCover photo by Elƒ´na ArƒÅja.\nThank you for reading my blog. You can reach out to me through my socials here:\n\nDiscord - ‚Äúlostsquid.‚Äù\nLinkedIn - /in/suchitg04/\n\nI hope to see you soon. Until then üëã"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Hey yo! Welcome to my blog peeps!",
    "section": "",
    "text": "Hey there! My name is Suchit. I am a freshman studying Information Science (or CS, IS is just a marketing term here XD) at RIT, Bangalore.\nIn this blog I hope to share my learnings and mostly tech stuff. My aim is to help the Suchit from a week or a month or an year ago, so that people going through the same problem/thoughts can gain better clarity. The added bonus is that I‚Äôll still be able to remember how I felt during that situation, therefore enabling me to provide you with help that‚Äôll actually help you.\nWith that said, you can expect my blogs to be short and have hints of GenZ humour here and there.\nFeel free to reach out to me through my socials:\n\nDiscord - ‚Äúlostsquid.‚Äù\n\nLinkedIn - /in/suchitg04/\n\nI hope to see you soon! Until then üëã"
  },
  {
    "objectID": "posts/tabular-model/index.html",
    "href": "posts/tabular-model/index.html",
    "title": "TabularModel Deep Dive",
    "section": "",
    "text": "In this post, I will attempt to explain the TabularModel class in fast.ai as part of the ‚ÄúFurther Research‚Äù assignment in chapter 9 of the fastbook.\nDisclaimer: This post is inspired by another wonderful blog post explaining the same.\nHere‚Äôs the code of TabularModel class pasted as is:\nclass TabularModel(Module):\n    \"Basic model for tabular data.\"\n    def __init__(self, \n        emb_szs:list, # Sequence of (num_embeddings, embedding_dim) for each categorical variable\n        n_cont:int, # Number of continuous variables\n        out_sz:int, # Number of outputs for final `LinBnDrop` layer\n        layers:list, # Sequence of ints used to specify the input and output size of each `LinBnDrop` layer\n        ps:float|MutableSequence=None, # Sequence of dropout probabilities for `LinBnDrop`\n        embed_p:float=0., # Dropout probability for `Embedding` layer\n        y_range=None, # Low and high for `SigmoidRange` activation \n        use_bn:bool=True, # Use `BatchNorm1d` in `LinBnDrop` layers\n        bn_final:bool=False, # Use `BatchNorm1d` on final layer\n        bn_cont:bool=True, # Use `BatchNorm1d` on continuous variables\n        act_cls=nn.ReLU(inplace=True), # Activation type for `LinBnDrop` layers\n        lin_first:bool=True # Linear layer is first or last in `LinBnDrop` layers\n    ):\n        ps = ifnone(ps, [0]*len(layers))\n        if not is_listy(ps): ps = [ps]*len(layers)\n        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(embed_p)\n        self.bn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont = n_emb,n_cont\n        sizes = [n_emb + n_cont] + layers + [out_sz]\n        actns = [act_cls for _ in range(len(sizes)-2)] + [None]\n        _layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first)\n                       for i,(p,a) in enumerate(zip(ps+[0.],actns))]\n        if y_range is not None: _layers.append(SigmoidRange(*y_range))\n        self.layers = nn.Sequential(*_layers)\n\n    def forward(self, x_cat, x_cont=None):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            if self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n        return self.layers(x)\nI will go through every (necessary) line of code but will refer you to this post if you want to see the outputs each line of code gives."
  },
  {
    "objectID": "posts/tabular-model/index.html#introduction",
    "href": "posts/tabular-model/index.html#introduction",
    "title": "TabularModel Deep Dive",
    "section": "",
    "text": "In this post, I will attempt to explain the TabularModel class in fast.ai as part of the ‚ÄúFurther Research‚Äù assignment in chapter 9 of the fastbook.\nDisclaimer: This post is inspired by another wonderful blog post explaining the same.\nHere‚Äôs the code of TabularModel class pasted as is:\nclass TabularModel(Module):\n    \"Basic model for tabular data.\"\n    def __init__(self, \n        emb_szs:list, # Sequence of (num_embeddings, embedding_dim) for each categorical variable\n        n_cont:int, # Number of continuous variables\n        out_sz:int, # Number of outputs for final `LinBnDrop` layer\n        layers:list, # Sequence of ints used to specify the input and output size of each `LinBnDrop` layer\n        ps:float|MutableSequence=None, # Sequence of dropout probabilities for `LinBnDrop`\n        embed_p:float=0., # Dropout probability for `Embedding` layer\n        y_range=None, # Low and high for `SigmoidRange` activation \n        use_bn:bool=True, # Use `BatchNorm1d` in `LinBnDrop` layers\n        bn_final:bool=False, # Use `BatchNorm1d` on final layer\n        bn_cont:bool=True, # Use `BatchNorm1d` on continuous variables\n        act_cls=nn.ReLU(inplace=True), # Activation type for `LinBnDrop` layers\n        lin_first:bool=True # Linear layer is first or last in `LinBnDrop` layers\n    ):\n        ps = ifnone(ps, [0]*len(layers))\n        if not is_listy(ps): ps = [ps]*len(layers)\n        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(embed_p)\n        self.bn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont = n_emb,n_cont\n        sizes = [n_emb + n_cont] + layers + [out_sz]\n        actns = [act_cls for _ in range(len(sizes)-2)] + [None]\n        _layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first)\n                       for i,(p,a) in enumerate(zip(ps+[0.],actns))]\n        if y_range is not None: _layers.append(SigmoidRange(*y_range))\n        self.layers = nn.Sequential(*_layers)\n\n    def forward(self, x_cat, x_cont=None):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            if self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n        return self.layers(x)\nI will go through every (necessary) line of code but will refer you to this post if you want to see the outputs each line of code gives."
  },
  {
    "objectID": "posts/tabular-model/index.html#init__",
    "href": "posts/tabular-model/index.html#init__",
    "title": "TabularModel Deep Dive",
    "section": "__init__",
    "text": "__init__"
  },
  {
    "objectID": "posts/tabular-model/index.html#ps",
    "href": "posts/tabular-model/index.html#ps",
    "title": "TabularModel Deep Dive",
    "section": "### ps",
    "text": "### ps\nThis line of code assigns an array of zeroes of length layers if ps=None.\nps = ifnone(ps, [0]*len(layers))\nI‚Äôm not entirely sure as to what this line does, but I think it creates an array of same valued ps of length layers if ps is a list.\nif not is_listy(ps): ps = [ps]*len(layers)"
  },
  {
    "objectID": "posts/tabular-model/index.html#embeds",
    "href": "posts/tabular-model/index.html#embeds",
    "title": "TabularModel Deep Dive",
    "section": "### embeds",
    "text": "### embeds\nself.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\nself.emb_drop = nn.Dropout(embed_p)\nself.bn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\nn_emb = sum(e.embedding_dim for e in self.embeds)\nself.n_emb,self.n_cont = n_emb,n_cont\nIf you notice emb_szs doesn‚Äôt have a default value assigned. This is because (AFAIK) TabularModel is not used directly, but used with tabular_learner. emb_szs are calculated inside tabular_learner using categorical data.\nemb_szs = get_emb_sz(dls.train_ds, {} if emb_szs is None else emb_szs)\nIn self.embeds = nn.ModuleList(...) embeddings (nn.Embedding) are created based on the sizes in emb_szs and are ‚Äúpackaged‚Äù together.\nself.emb_drop = nn.Dropout(...) (acc. to the documentation) randomly zeroes out the input tensors based on the probabilities in embed_p.\nI have no clue about nn.BatchNorm1d üòÖ.\nn_emb gets assigned the total number of embedding dimensions.\nemb_szs = [(4, 2), (5, 3)]\nembeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\nn_emb = sum(e.embedding_dim for e in embeds)\nprint(n_emb)\nModuleList(\n  (0): Embedding(4, 2)\n  (1): Embedding(5, 3)\n)"
  },
  {
    "objectID": "posts/tabular-model/index.html#layers",
    "href": "posts/tabular-model/index.html#layers",
    "title": "TabularModel Deep Dive",
    "section": "### layers",
    "text": "### layers\nsizes = [n_emb + n_cont] + layers + [out_sz]\nactns = [act_cls for _ in range(len(sizes)-2)] + [None]\n_layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first)\nfor i,(p,a) in enumerate(zip(ps+[0.],actns))]\nif y_range is not None: _layers.append(SigmoidRange(*y_range))\nself.layers = nn.Sequential(*_layers)\nAssuming n_emb=5, n_cont=1, layers=[100, 50], out_sz=1, then sizes = [6, 100, 50, 1].\nactns holds the activation function to be used between each layers. In this case, it holds:\n[ReLU(inplace=True), ReLU(inplace=True), None]\nIn the 3rd line, _layers gets assigned a ‚Äúgrouping‚Äù of BatchNorm1d, Dropout and Linear layers. This is where we understand why sizes is assigned such values. The first two arguments passed to LinBnDrop specify the number of input and output neurons respectively.\nThe activations are passed through act=a where a is an object enumerated from the zip of ps and actns. The first ReLU activation is applied between layers of size 6 and 100, and the second between layers of size 100 and 50. actns has its third element as None because there isn‚Äôt supposed to be any activation function between the last two layers (of size 50 and 1, in this case).\nIn the same line, there‚Äôs a bn parameter that says whether to use batchnorm after the current layer (i). If use_bn is False, then it isn‚Äôt used. If it is True, and one of i!=len(actns)-1 (if i!=2 or if i, the current layer, isn‚Äôt the 2nd to last layer) or bn_final is True, then batchnorm is used.\nI refer you to Vishal‚Äôs post I put in the disclaimer earlier if you aren‚Äôt satisfied with this explanation.\nThe next line appends a SigmoidRange function that limits the output values within the specified range. And finally, the last line wraps all the layers in nn.Sequential."
  },
  {
    "objectID": "posts/tabular-model/index.html#forward",
    "href": "posts/tabular-model/index.html#forward",
    "title": "TabularModel Deep Dive",
    "section": "forward",
    "text": "forward"
  },
  {
    "objectID": "posts/tabular-model/index.html#prepping-categorical-variables",
    "href": "posts/tabular-model/index.html#prepping-categorical-variables",
    "title": "TabularModel Deep Dive",
    "section": "### Prepping categorical variables",
    "text": "### Prepping categorical variables\nx = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\nx = torch.cat(x, 1)\nx = self.emb_drop(x)\nFirstly, embeddings for x_cat are created and stored in x. Then they are concanetated along the columns into a single tensor. Dropout layers are added in the next line based on embed_p."
  },
  {
    "objectID": "posts/tabular-model/index.html#prepping-continous-variables",
    "href": "posts/tabular-model/index.html#prepping-continous-variables",
    "title": "TabularModel Deep Dive",
    "section": "### Prepping continous variables",
    "text": "### Prepping continous variables\nif self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\nx = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\nIn the first line, x_cont = self.bn_cont(x_cont) is run if bn_cont (a parameter to TabularModel) is not None. Next, x_cont is concatenated to x which has the categorical embeddings if self.n_emb is not 0. Otherwise, x_cont is assigned to x.\nFinally, x is passed to self.layers which is essentially the model with BatchNorm1d, Dropout and Linear layers and the output is returned."
  },
  {
    "objectID": "posts/tabular-model/index.html#conclusion",
    "href": "posts/tabular-model/index.html#conclusion",
    "title": "TabularModel Deep Dive",
    "section": "Conclusion",
    "text": "Conclusion\nThis was a challenging exercise that filled gaps in my understanding of python and also taught me how Neural Networks are used on tabular datasets. Again, this post helped me a lot with my understanding.\nThank you for reading my blog. You can reach out to me through my socials here:\n\nDiscord - ‚Äúlostsquid.‚Äù\nLinkedIn - /in/suchitg04/\n\nI hope to see you soon. Until then üëã"
  },
  {
    "objectID": "posts/tvdesktopclassifier/index.html",
    "href": "posts/tvdesktopclassifier/index.html",
    "title": "Train your first image classifier (AI) model!",
    "section": "",
    "text": "Want to get hands-on experience with AI? If so, this is the perfect tutorial for you! Pre-requisites for this tutorial:\n\nbeginner-level python\nbe able to use jupyter notebooks\na Kaggle account (go create one duh)\n\nThat‚Äôs all! If you don‚Äôt know how to use Jupyter notebooks, click here for a quick tutorial on both Jupyter notebooks and Kaggle.\nSo what exactly will we be creating in this tutorial? We are gonna train a Deep Learning model to identify if a given image is a CRT TV or a flat screen TV or a desktop monitor. We will be using a beginner friendly and widely used library called fastai. Sounds damn cool, at least to me :)\nNote 1: You need not break your head over what each line of code does. This tutorial is meant to give you impetus to delve into Deep Learning and a top level overview of it‚Äôs power :)\nNote 2: GPU needs to be enabled for this tutorial or you‚Äôll be spending hours training the model üòÇ. Specifically enable GPU P100. Also, to use GPUs you need to have your phone number verified, so go do that if you haven‚Äôt yet."
  },
  {
    "objectID": "posts/tvdesktopclassifier/index.html#introduction",
    "href": "posts/tvdesktopclassifier/index.html#introduction",
    "title": "Train your first image classifier (AI) model!",
    "section": "",
    "text": "Want to get hands-on experience with AI? If so, this is the perfect tutorial for you! Pre-requisites for this tutorial:\n\nbeginner-level python\nbe able to use jupyter notebooks\na Kaggle account (go create one duh)\n\nThat‚Äôs all! If you don‚Äôt know how to use Jupyter notebooks, click here for a quick tutorial on both Jupyter notebooks and Kaggle.\nSo what exactly will we be creating in this tutorial? We are gonna train a Deep Learning model to identify if a given image is a CRT TV or a flat screen TV or a desktop monitor. We will be using a beginner friendly and widely used library called fastai. Sounds damn cool, at least to me :)\nNote 1: You need not break your head over what each line of code does. This tutorial is meant to give you impetus to delve into Deep Learning and a top level overview of it‚Äôs power :)\nNote 2: GPU needs to be enabled for this tutorial or you‚Äôll be spending hours training the model üòÇ. Specifically enable GPU P100. Also, to use GPUs you need to have your phone number verified, so go do that if you haven‚Äôt yet."
  },
  {
    "objectID": "posts/tvdesktopclassifier/index.html#installing-modules",
    "href": "posts/tvdesktopclassifier/index.html#installing-modules",
    "title": "Train your first image classifier (AI) model!",
    "section": "2. Installing modules",
    "text": "2. Installing modules\nHere, we will be installing/updating the python modules necessary for this tutorial.\nRun the two following code blocks to do so.\n::: {#cell-3 .cell _kg_hide-output=‚Äòfalse‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2023-05-29T15:14:07.221608Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2023-05-29T15:14:07.220895Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2023-05-29T15:14:26.318729Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2023-05-29T15:14:26.317485Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2023-05-29T15:14:07.221548Z‚Äù}‚Äô trusted=‚Äòtrue‚Äô execution_count=3}\nimport os\nfrom fastcore.all import *\nimport urllib.request\nfrom fastai.vision.all import *\nfrom fastdownload import download_url\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle:\n    !pip install -Uqq fastai duckduckgo_search\n\nfrom duckduckgo_search import ddg_images\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npandas-profiling 3.6.2 requires requests&lt;2.29,&gt;=2.24.0, but you have requests 2.31.0 which is incompatible.\nlibrosa 0.10.0.post2 requires soundfile&gt;=0.12.1, but you have soundfile 0.11.0 which is incompatible.\napache-beam 2.44.0 requires dill&lt;0.3.2,&gt;=0.3.1.1, but you have dill 0.3.6 which is incompatible.\n\n:::"
  },
  {
    "objectID": "posts/tvdesktopclassifier/index.html#glimpsing-our-data",
    "href": "posts/tvdesktopclassifier/index.html#glimpsing-our-data",
    "title": "Train your first image classifier (AI) model!",
    "section": "3. Glimpsing our data",
    "text": "3. Glimpsing our data\nWhat is AI without data? Data is arguably the most important thing in the field of AI/Data Science. AI models are trained using tons and tons of data. Let‚Äôs have a look at how our data looks like.\nIn the first block, we search and retrieve one URL for an image of a flat screen TV. We then download the image from the retrieved URL and open the image and repeat the process for a CRT TV in the subsequent code blocks.\n\ndef search_images(term, max_images=40):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n::: {#cell-6 .cell _kg_hide-input=‚Äòfalse‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2023-05-29T15:14:42.535536Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2023-05-29T15:14:42.535063Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2023-05-29T15:14:43.824537Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2023-05-29T15:14:43.823463Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2023-05-29T15:14:42.535494Z‚Äù}‚Äô trusted=‚Äòtrue‚Äô execution_count=5}\nurls = search_images('flat screen tv', max_images=1)\nurls[0]\n\nSearching for 'flat screen tv'\n\n\n/opt/conda/lib/python3.7/site-packages/duckduckgo_search/compat.py:60: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n/opt/conda/lib/python3.7/site-packages/duckduckgo_search/compat.py:64: UserWarning: parameter page is deprecated\n  warnings.warn(\"parameter page is deprecated\")\n/opt/conda/lib/python3.7/site-packages/duckduckgo_search/compat.py:66: UserWarning: parameter max_results is deprecated\n  warnings.warn(\"parameter max_results is deprecated\")\n\n\n'http://s4msungtelevision32.files.wordpress.com/2013/01/flat-screen-televisions.jpg'\n\n:::\n\ndest = 'flatscreentv.jpg'\n# Trying two libraries because they both are working erratically for me\n# try:\n#     download_url(urls[0], dest, show_progress=False)\n#     print(\"hi\")\n# except:\nurllib.request.urlretrieve(urls[0], dest)\n\n('flatscreentv.jpg', &lt;http.client.HTTPMessage at 0x7e8f7900d090&gt;)\n\n\n\nImage.open(dest).to_thumb(256, 256)\n\n\n\n\n\n\n\n\n\ntry:\n    download_url(search_images('crt tv', max_images=1)[0], 'crttv.jpg', show_progress=False)\nexcept:\n    urllib.request.urlretrieve(search_images('crt tv', max_images=1)[0], 'crttv.jpg')\n\nImage.open('crttv.jpg').to_thumb(256, 256)\n\nSearching for 'crt tv'"
  },
  {
    "objectID": "posts/tvdesktopclassifier/index.html#downloading-images",
    "href": "posts/tvdesktopclassifier/index.html#downloading-images",
    "title": "Train your first image classifier (AI) model!",
    "section": "4. Downloading images",
    "text": "4. Downloading images\nLet us now download a bunch of images to train our model and make them ready to be ‚Äúfed‚Äù into the model.\n\n# Downloading images into their respective directories\nsearches = 'flat screen tv', 'crt tv', 'desktop monitor'\npath = Path('tv_or_desktop')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(o))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'flat screen tv'\nSearching for 'crt tv'\nSearching for 'desktop monitor'\n\n\n\n# Check and remove unopenable images\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n7\n\n\n\n# Loading the data\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=9)"
  },
  {
    "objectID": "posts/tvdesktopclassifier/index.html#training-our-model",
    "href": "posts/tvdesktopclassifier/index.html#training-our-model",
    "title": "Train your first image classifier (AI) model!",
    "section": "5. Training our model!",
    "text": "5. Training our model!\nThis is it people. ‚Äôtis time to train our model! For this example we use a model called ResNet18 with 18 layers. ResNet18 is pre-trained on ImageNet dataset. Therefore, we need not train it again, rather we fine tune it to recognize images from our dataset, i.e., flatscreen TV, CRT TV and desktop monitors.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(5)\n\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.542415\n1.480012\n0.523810\n00:06\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.913363\n1.170745\n0.476190\n00:01\n\n\n1\n0.876068\n0.878507\n0.333333\n00:01\n\n\n2\n0.637958\n0.758396\n0.238095\n00:01\n\n\n3\n0.499402\n0.701759\n0.190476\n00:01\n\n\n4\n0.409540\n0.639917\n0.190476\n00:01"
  },
  {
    "objectID": "posts/tvdesktopclassifier/index.html#classifying-images",
    "href": "posts/tvdesktopclassifier/index.html#classifying-images",
    "title": "Train your first image classifier (AI) model!",
    "section": "6. Classifying images",
    "text": "6. Classifying images\nCongratulations! You have trained your first (ig ü§∑) image classification model! Let us now put it to test and try classifying some images.\n\n# Predicting an image we had downloaded earlier\nlearn.predict(PILImage.create('crttv.jpg'))\n\n\n\n\n\n\n\n\n('crt tv', tensor(0), tensor([9.9726e-01, 5.5894e-04, 2.1807e-03]))\n\n\n\n# Predicting an image we had downloaded earlier\nlearn.predict(PILImage.create('flatscreentv.jpg'))\n\n\n\n\n\n\n\n\n('desktop monitor', tensor(1), tensor([6.4813e-04, 9.9257e-01, 6.7823e-03]))\n\n\n\n# download_url(search_images('desktop monitor', max_images=1)[0], dest='desktopmonitor.jpg', show_progress=False)\ntry:\n    download_url(search_images('desktop monitor', max_images=1)[0], 'desktopmonitor.jpg', show_progress=False)\nexcept:\n    urllib.request.urlretrieve(search_images('desktop monitor', max_images=1)[0], 'desktopmonitor.jpg')\n\nImage.open('desktopmonitor.jpg').to_thumb(256, 256)\n\nSearching for 'desktop monitor'\n\n\n\n\n\n\n\n\n\n\nlearn.predict(PILImage.create('desktopmonitor.jpg'))\n\n\n\n\n\n\n\n\n('desktop monitor', tensor(1), tensor([5.4398e-05, 9.9916e-01, 7.8518e-04]))\n\n\nWell, that‚Äôs not bad for our first model. It has a decent accuracy. For me, it got 2/3 predictions right.\nIf you have any questions don‚Äôt hesitate to message me on discord. And lastly, here‚Äôs the link to my notebook if you wannaplay around with it (click on the ‚Äúcopy and edit button‚Äù).\nThank you for reading my blog. You can reach out to me through my socials here:\n\nDiscord - ‚Äúlostsquid.‚Äù\nLinkedIn - /in/suchitg04/\n\nI hope to see you soon. Until then üëã"
  },
  {
    "objectID": "bits.html",
    "href": "bits.html",
    "title": "Bits",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nResource preemption, and laser focus.\n\n\nI can multi-task so well ü§°.\n\n\n\n\n\nNov 20, 2024\n\n\nSuchit G\n\n\n\n\n\n\n\n\n\n\n\n\nTest-time compute, but for humans.\n\n\nImagine writing lol. No, actually, imagine writing.\n\n\n\n\n\nNov 11, 2024\n\n\nSuchit G\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "bits/focus_and_resource_preemption/index.html",
    "href": "bits/focus_and_resource_preemption/index.html",
    "title": "Resource preemption, and laser focus.",
    "section": "",
    "text": "Resource preemption is an important concept in OS. No, I‚Äôm not here to lecture you about it.\nTo put it simply, the processor pauses the current process and attends to some other process, ideally remembering to come back to the original process.\nIt is a nifty algorithm. But what about task preemption in humans?\nImagine I‚Äôm working hard on a problem ‚Äî programming, math or anything else. I remember another task that I had to do. Maybe it‚Äôs more important, maybe it isn‚Äôt ‚Äì we suck at judging that. I switch tasks, preempting the original one.\nNow, the probability of me returning promptly to the original task is lower than I‚Äôd like it. Often, I detour through some other task. In other words, our ability to hold the state of the preempted process is remarkably poor.\nThis hampers my productivity quite a lot. In fact, a lot of distractions, be it dopamine inducing or any other actual tasks, neatly fit into this paradigm.\nThis highlights the need for laser focus. If you are to complete any given task in optimal time and not half-ass it, ‚Äúlaser focus‚Äù is essential.\n‚Äôtis a fancy term indeed, but by laser focus I mean being focused on a single goal, task, objective, etc. Maybe there are different ways of thinking about focus in different contexts, I don‚Äôt know. I‚Äôm yet to figure it out."
  }
]