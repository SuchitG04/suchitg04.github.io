---
title: Let's Backprop
toc: true
author: Suchit G
description: A guide to understanding backprop intuitively.
categories:
    - Neural Networks
    - Math
draft: true
---

To get the most out of this guide, you'll need to know what layers are in neural networks. That's all. That's the only peculiar requirement to understand this post.

## Why Backprop?

A neural network or, more generally, a deep learning model learns based on how different it predictions are from the actual values in the dataset. It does this with the help of a loss function. But how does a loss function help the model learn? This is where backpropagation is needed.

A TLDR of backprop would be: Backprop tells every parameter (weights and biases) in what direction (positive or negatvive) it should change to make the model's predictions closer to the actual values.

## Primer on Neural Networks

I'll start by giving my intuition about neural networks. I percieve it as a bunch of numbers being transformed from one vector space to another. In simpler words, it's just a vector whose dimensions are being changed using "layers" (which IMO is a bad abstraction to give to beginners trying to understand neural networks).

We'll use a sample input vector of 10 dimensions. Our input vector looks something like this:
$$
\hspace{1mm}\text{x} = \begin{bmatrix} 2&2&1&4&3&2&0&1&4&4 \end{bmatrix}
$$
which is just a flattened version of the matrix representation of an image.

Now let's define a simple neural network to show how the input is "forwarded" through the network. We'll be using a familiar PyTorch code snippet for that.

```python
neural_network = nn.Sequential(
    nn.Linear(10, 5),
    nn.Linear(5, 6),
)
```

The first "layer" looks like this.

![](nn1.svg){height=600 fig-align="center"}

But what does it do exactly? Consider this weight matrix for the first layer filled out randomly:
$$
\text{W}_1 =
\begin{bmatrix}
3 & 7 & 2 & 8 & 1 \\
0 & 6 & 4 & 9 & 5 \\
5 & 2 & 1 & 0 & 3 \\
8 & 7 & 6 & 4 & 2 \\
9 & 1 & 3 & 7 & 0 \\
4 & 8 & 5 & 2 & 9 \\
6 & 0 & 7 & 3 & 8 \\
1 & 9 & 4 & 6 & 2 \\
2 & 5 & 8 & 1 & 7 \\
7 & 3 & 0 & 5 & 4
\end{bmatrix}
$$

The inputs (of dimension 10) are multiplied with $\text{W}_1$. Multiplying a matrix of dimensions $\text{1} \times \text{10} \hspace{1mm}$ with a matrix of dimensions $\text{10} \times \text{5}\hspace{1mm}$ yields a matrix of dimensions $\hspace{1mm} \text{1} \times \text{5}$. This is essentially transforming the input vector into a vector of a different vector space. Therefore, the resultant matrix is
$$
\text{x}_1 =
\begin{bmatrix}
115 & 112 & 92 & 105 & 87
\end{bmatrix}
$$

If you squint hard enough, you can figure out how this translates to the classical representation of a neural network (as depicted in the image above). Each neuron in the input layer has 5 lines coming out of it. These five lines of, for example, the first neuron, represents the 5 weight values in the first row of $\text{W}_1$, and so on.

Let's do this for the "2nd layer" as well.

Presented below is it's representations as neurons:

![](nn2.svg){height=400 fig-align="center"}

The weight matrix for the 2nd layer would look like this:
$$
\text{W}_2 =
\begin{bmatrix}
1 & 7 & 3 & 8 & 2 & 6 \\
4 & 0 & 9 & 5 & 1 & 7 \\
8 & 3 & 2 & 4 & 6 & 0 \\
5 & 9 & 7 & 1 & 3 & 2 \\
6 & 2 & 0 & 7 & 8 & 4
\end{bmatrix}
$$

By now, you might have gotten the intuition that weights are just the representation of lines in the above figure. So, the 2nd pass would be like transforming the 5-dimensional vector $\text{x}_1$ to a 6-dimensional vector.

To reiterate, multiplying the 5-dimensional vector (matrix of dimensions $\text{1} \times \text{5}\hspace{1mm}$) $\text{x}_1$ with $\text{W}_2$ that has dimensions $\text{5} \times \text{6}$ results in a matrix of dimensions $\text{1} \times \text{6}$, like so:
$$
\text{x}_2 =
\begin{bmatrix}
2346 & 2200 && 2272 & 1905 & 2032
\end{bmatrix}
$$

One important thing we didn't discuss are activation functions between layers or between transformations. These functions are basically applied element-wise to the resultant vectors after multiplication with the weight matrices. As we'll see later, these functions are what give meaning to "Multi-layer neural networks" (popularly called Multi-Layered Perceptrons) because without them, the whole network (like in our example above, without any activation functions) would be equivalent to just a single transformation (in other words, a single layer).

That about concludes what I had to say about neural networks. You now have a more intuitive understanding of neural networks than the commonly found 'layers' terminology. Moreover, I encourage you to gain further intuitions on matrix multiplication. I found [this](http://matrixmultiplication.xyz) and [this](https://pytorch.org/blog/inside-the-matrix) helpful.

## What is Differentiation akshually?

Good question. I myself didn't understand it completely until recently. I knew formulae like $\frac{\mathrm{d} x^2}{\mathrm{d} x} = \text{2x}$, and that was about it.



## Applying Backprop on a simple network

### Understanding Vector differentiation

[ - define the network (write down its layers) - do backprop manually for some test data - include notes on why learning rate is used]

## Conclusion
